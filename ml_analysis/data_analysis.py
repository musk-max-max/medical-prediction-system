#!/usr/bin/env python3
"""
åŒ»ç–—é¢„æµ‹ç³»ç»Ÿ - æ•°æ®åˆ†æå’Œé¢„å¤„ç†
Framingham Heart Study æ•°æ®åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_and_explore_data(file_path):
    """åŠ è½½å’Œæ¢ç´¢æ•°æ®"""
    print("ğŸ” åŠ è½½æ•°æ®...")
    df = pd.read_csv(file_path)
    
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"ğŸ“‹ åˆ—æ•°: {df.shape[1]}")
    print(f"ğŸ“ è¡Œæ•°: {df.shape[0]}")
    
    print("\nğŸ“‘ æ•°æ®ä¿¡æ¯:")
    print(df.info())
    
    print("\nğŸ“ˆ æ•°å€¼å‹ç‰¹å¾æè¿°ç»Ÿè®¡:")
    print(df.describe())
    
    return df

def analyze_missing_values(df):
    """åˆ†æç¼ºå¤±å€¼"""
    print("\nğŸ” ç¼ºå¤±å€¼åˆ†æ:")
    missing = df.isnull().sum()
    missing_percent = (missing / len(df)) * 100
    missing_df = pd.DataFrame({
        'ç¼ºå¤±æ•°é‡': missing,
        'ç¼ºå¤±æ¯”ä¾‹(%)': missing_percent
    })
    missing_df = missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0].sort_values('ç¼ºå¤±æ•°é‡', ascending=False)
    
    if not missing_df.empty:
        print(missing_df)
        
        # å¯è§†åŒ–ç¼ºå¤±å€¼
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        missing_df['ç¼ºå¤±æ•°é‡'].plot(kind='bar')
        plt.title('ç¼ºå¤±å€¼æ•°é‡')
        plt.xticks(rotation=45)
        
        plt.subplot(1, 2, 2)
        missing_df['ç¼ºå¤±æ¯”ä¾‹(%)'].plot(kind='bar')
        plt.title('ç¼ºå¤±å€¼æ¯”ä¾‹ (%)')
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.savefig('missing_values_analysis.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š ç¼ºå¤±å€¼åˆ†æå›¾å·²ä¿å­˜: missing_values_analysis.png")
    else:
        print("âœ… æ²¡æœ‰ç¼ºå¤±å€¼")
    
    return missing_df

def analyze_target_variable(df):
    """åˆ†æç›®æ ‡å˜é‡"""
    # å€™é€‰ç›®æ ‡å˜é‡
    potential_targets = ['CVD', 'MI_FCHD', 'ANYCHD', 'STROKE', 'DEATH', 'ANGINA', 'HOSPMI']
    
    print("\nğŸ¯ ç›®æ ‡å˜é‡åˆ†æ:")
    for target in potential_targets:
        if target in df.columns:
            value_counts = df[target].value_counts()
            print(f"\n{target}:")
            print(value_counts)
            print(f"æ‚£ç—…ç‡: {(value_counts.get(1, 0) / len(df)) * 100:.2f}%")

def correlation_analysis(df):
    """ç›¸å…³æ€§åˆ†æ"""
    print("\nğŸ”— ç›¸å…³æ€§åˆ†æ:")
    
    # é€‰æ‹©æ•°å€¼å‹åˆ—
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_cols) > 1:
        correlation_matrix = df[numeric_cols].corr()
        
        plt.figure(figsize=(15, 12))
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', 
                   center=0, square=True, linewidths=0.5)
        plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')
        plt.tight_layout()
        plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š ç›¸å…³æ€§çƒ­åŠ›å›¾å·²ä¿å­˜: correlation_heatmap.png")
        
        # æ‰¾å‡ºé«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
        high_corr = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_val = abs(correlation_matrix.iloc[i, j])
                if corr_val > 0.7:  # é«˜ç›¸å…³æ€§é˜ˆå€¼
                    high_corr.append({
                        'Feature1': correlation_matrix.columns[i],
                        'Feature2': correlation_matrix.columns[j],
                        'Correlation': correlation_matrix.iloc[i, j]
                    })
        
        if high_corr:
            print("\nâš ï¸ é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ (|r| > 0.7):")
            for pair in high_corr:
                print(f"{pair['Feature1']} - {pair['Feature2']}: {pair['Correlation']:.3f}")

def feature_distributions(df):
    """ç‰¹å¾åˆ†å¸ƒåˆ†æ"""
    print("\nğŸ“Š ç‰¹å¾åˆ†å¸ƒåˆ†æ:")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # ç§»é™¤IDåˆ—
    numeric_cols = [col for col in numeric_cols if 'ID' not in col.upper()]
    
    if len(numeric_cols) > 0:
        n_cols = 4
        n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
        
        plt.figure(figsize=(20, 5 * n_rows))
        
        for i, col in enumerate(numeric_cols[:16]):  # é™åˆ¶æ˜¾ç¤ºå‰16ä¸ªç‰¹å¾
            plt.subplot(n_rows, n_cols, i + 1)
            df[col].hist(bins=30, alpha=0.7)
            plt.title(f'{col} åˆ†å¸ƒ')
            plt.xlabel(col)
            plt.ylabel('é¢‘æ•°')
        
        plt.tight_layout()
        plt.savefig('feature_distributions.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š ç‰¹å¾åˆ†å¸ƒå›¾å·²ä¿å­˜: feature_distributions.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ åŒ»ç–—é¢„æµ‹ç³»ç»Ÿ - æ•°æ®åˆ†æ")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®
    df = load_and_explore_data('../frmgham_data.csv')
    
    # ç¼ºå¤±å€¼åˆ†æ
    missing_info = analyze_missing_values(df)
    
    # ç›®æ ‡å˜é‡åˆ†æ
    analyze_target_variable(df)
    
    # ç›¸å…³æ€§åˆ†æ
    correlation_analysis(df)
    
    # ç‰¹å¾åˆ†å¸ƒåˆ†æ
    feature_distributions(df)
    
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®ä¿¡æ¯
    print("\nğŸ’¾ ä¿å­˜æ•°æ®æ‘˜è¦...")
    with open('data_summary.txt', 'w', encoding='utf-8') as f:
        f.write("åŒ»ç–—é¢„æµ‹ç³»ç»Ÿæ•°æ®æ‘˜è¦\n")
        f.write("=" * 30 + "\n\n")
        f.write(f"æ•°æ®å½¢çŠ¶: {df.shape}\n")
        f.write(f"ç‰¹å¾æ•°é‡: {df.shape[1]}\n")
        f.write(f"æ ·æœ¬æ•°é‡: {df.shape[0]}\n\n")
        
        f.write("åˆ—å:\n")
        for col in df.columns:
            f.write(f"- {col}\n")
    
    print("âœ… æ•°æ®åˆ†æå®Œæˆ!")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - missing_values_analysis.png")
    print("  - correlation_heatmap.png") 
    print("  - feature_distributions.png")
    print("  - data_summary.txt")

if __name__ == "__main__":
    main() 