#!/usr/bin/env python3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, accuracy_score
import joblib
import json

print('ğŸ¥ å¼—é›·æ˜æ±‰çºµå‘å¿ƒè¡€ç®¡ç–¾ç—…é£é™©é¢„æµ‹æ¨¡å‹')
print('åŸºäºæ­£ç¡®ç†è§£çš„çºµå‘æ•°æ®ç»“æ„')
print('=' * 60)

# åŠ è½½æ•°æ®
df = pd.read_csv('../frmgham_data.csv')
print(f'æ€»è®°å½•æ•°: {len(df)}')
print(f'ç‹¬ç‰¹æ‚£è€…æ•°: {df["RANDID"].nunique()}')

# ç†è§£æ•°æ®ç»“æ„
print(f'\nPERIODåˆ†å¸ƒ:')
print(df['PERIOD'].value_counts().sort_index())

# åˆ†æCVDå­—æ®µçš„å«ä¹‰
print(f'\nCVDå­—æ®µåˆ†æ:')
print(f'CVD=1çš„æ‚£è€…æ•°: {(df["CVD"]==1).sum()}')
print(f'CVD=1çš„æ¯”ä¾‹: {(df["CVD"]==1).mean():.3f}')

# æŸ¥çœ‹å‡ ä¸ªæ‚£è€…çš„çºµå‘æ•°æ®
print(f'\næŸ¥çœ‹å‰3ä¸ªæ‚£è€…çš„çºµå‘æ•°æ®ç¤ºä¾‹:')
for randid in df['RANDID'].unique()[:3]:
    patient_data = df[df['RANDID'] == randid][['RANDID', 'PERIOD', 'AGE', 'CVD', 'PREVCHD', 'TIMECVD']].sort_values('PERIOD')
    print(f'æ‚£è€… {randid}:')
    print(patient_data.to_string(index=False))
    print()

# åŸºçº¿é¢„æµ‹æ¨¡å‹ï¼šä½¿ç”¨ç¬¬ä¸€æ¬¡ä½“æ£€æ•°æ®é¢„æµ‹CVDé£é™©
print('ğŸ“Š åˆ›å»ºåŸºçº¿é¢„æµ‹æ¨¡å‹ï¼ˆä½¿ç”¨ç¬¬ä¸€æ¬¡ä½“æ£€æ•°æ®ï¼‰')
baseline_data = df[df['PERIOD'] == 1].copy()

# é€‰æ‹©é‡è¦çš„å¿ƒè¡€ç®¡é£é™©å› ç´ 
risk_factors = [
    'SEX', 'AGE', 'TOTCHOL', 'SYSBP', 'DIABP', 
    'CURSMOKE', 'BMI', 'DIABETES', 'PREVCHD', 
    'PREVMI', 'PREVSTRK'
]

available_features = [col for col in risk_factors if col in baseline_data.columns]
X = baseline_data[available_features].copy()
y = baseline_data['CVD']  # CVD=1è¡¨ç¤ºåœ¨ç ”ç©¶æœŸé—´æ›¾æ‚£å¿ƒè¡€ç®¡ç–¾ç—…

# ç®€å•ç‰¹å¾å·¥ç¨‹
if 'SYSBP' in X.columns and 'DIABP' in X.columns:
    X['PULSE_PRESSURE'] = X['SYSBP'] - X['DIABP']

print(f'ç‰¹å¾æ•°é‡: {X.shape[1]}, æ ·æœ¬æ•°é‡: {X.shape[0]}')
print(f'CVDæ‚£ç—…ç‡: {y.mean():.3f}')

# å¤„ç†ç¼ºå¤±å€¼å’Œæ ‡å‡†åŒ–
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# è®­ç»ƒæ¨¡å‹
models = {
    'Logistic_Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random_Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    accuracy = accuracy_score(y_test, model.predict(X_test))
    auc = roc_auc_score(y_test, y_pred_proba)
    results[name] = {'accuracy': accuracy, 'auc': auc, 'model': model}
    print(f'{name}: å‡†ç¡®ç‡={accuracy:.4f}, AUC={auc:.4f}')

# ä¿å­˜æœ€ä½³æ¨¡å‹
best_model_name = max(results.keys(), key=lambda x: results[x]['auc'])
best_model = results[best_model_name]

# ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶
joblib.dump(best_model['model'], 'longitudinal_baseline_model.pkl')
joblib.dump(scaler, 'longitudinal_baseline_scaler.pkl') 
joblib.dump(imputer, 'longitudinal_baseline_imputer.pkl')

# ä¿å­˜ç‰¹å¾åç§°
feature_names = available_features + ['PULSE_PRESSURE']
with open('longitudinal_baseline_features.json', 'w') as f:
    json.dump(feature_names, f, indent=2)

# ä¿å­˜æ¨¡å‹ä¿¡æ¯
model_info = {
    'model_name': best_model_name,
    'model_type': 'baseline_longitudinal',
    'auc_score': best_model['auc'],
    'accuracy': best_model['accuracy'],
    'features': feature_names,
    'description': 'ä½¿ç”¨ç¬¬ä¸€æ¬¡ä½“æ£€æ•°æ®é¢„æµ‹æ‚£è€…åœ¨ç ”ç©¶æœŸé—´æ˜¯å¦ä¼šæ‚£CVD'
}

with open('longitudinal_baseline_info.json', 'w', encoding='utf-8') as f:
    json.dump(model_info, f, ensure_ascii=False, indent=2)

print(f'\nğŸ† æœ€ä½³åŸºçº¿æ¨¡å‹: {best_model_name}')
print(f'ğŸ¯ AUCåˆ†æ•°: {best_model["auc"]:.4f}')
print(f'ğŸ“ˆ å‡†ç¡®ç‡: {best_model["accuracy"]:.4f}')
print('âœ… çºµå‘åŸºçº¿æ¨¡å‹å·²ä¿å­˜')

print('\nğŸ’¡ æ•°æ®ç†è§£æ€»ç»“:')
print('1. æ•°æ®åŒ…å«4434åæ‚£è€…ï¼Œæœ€å¤š3æ¬¡ä½“æ£€è®°å½•ï¼ˆPERIOD 1-3ï¼‰')
print('2. CVD=1è¡¨ç¤ºæ‚£è€…åœ¨æ•´ä¸ªç ”ç©¶æœŸé—´æ›¾ç»æ‚£å¿ƒè¡€ç®¡ç–¾ç—…')
print('3. PREVCHD=1è¡¨ç¤ºåœ¨å½“å‰ä½“æ£€æ—¶å·²ç»æ‚£æœ‰å† å¿ƒç—…')
print('4. TIMECVDè¡¨ç¤ºä»ç¬¬ä¸€æ¬¡ä½“æ£€åˆ°é¦–æ¬¡CVDè¯Šæ–­çš„æ—¶é—´é—´éš”ï¼ˆå°æ—¶ï¼‰')
print('5. åŸºçº¿æ¨¡å‹ä½¿ç”¨ç¬¬ä¸€æ¬¡ä½“æ£€æ•°æ®é¢„æµ‹æ‚£è€…æœªæ¥CVDé£é™©')
print('6. è¿™æ˜¯ä¸€ä¸ªå‰ç»æ€§é˜Ÿåˆ—ç ”ç©¶çš„æ­£ç¡®å»ºæ¨¡æ–¹æ³•')

print('\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:')
print('- longitudinal_baseline_model.pkl (åŸºçº¿æ¨¡å‹)')
print('- longitudinal_baseline_scaler.pkl (æ ‡å‡†åŒ–å™¨)')  
print('- longitudinal_baseline_imputer.pkl (ç¼ºå¤±å€¼å¤„ç†å™¨)')
print('- longitudinal_baseline_features.json (ç‰¹å¾åç§°)')
print('- longitudinal_baseline_info.json (æ¨¡å‹ä¿¡æ¯)') 