#!/usr/bin/env python3
"""
å¼—é›·æ˜æ±‰å¿ƒè¡€ç®¡ç–¾ç—…24å¹´éšè®¿é¢„æµ‹æ¨¡å‹
åŸºäºæ­£ç¡®ç†è§£çš„TIMECVDå­—æ®µå«ä¹‰ï¼š
- TIMECVD = 8766å¤©: æ‚£è€…åœ¨24å¹´éšè®¿æœŸå†…æœªæ‚£CVD (åˆ å¤±æ•°æ®)  
- TIMECVD < 8766å¤©: æ‚£è€…æ‚£CVDçš„å…·ä½“æ—¶é—´ç‚¹ (äº‹ä»¶æ•°æ®)
- å•ä½ï¼šå¤©
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, accuracy_score, classification_report
import joblib
import json

print('ğŸ¥ å¼—é›·æ˜æ±‰å¿ƒè¡€ç®¡ç–¾ç—…24å¹´éšè®¿é¢„æµ‹æ¨¡å‹')
print('åŸºäºæ­£ç¡®ç†è§£çš„TIMECVDå­—æ®µå«ä¹‰ï¼ˆå•ä½ï¼šå¤©ï¼‰')
print('=' * 60)

# åŠ è½½æ•°æ®
df = pd.read_csv('../frmgham_data.csv')
print(f'æ€»è®°å½•æ•°: {len(df)}')
print(f'ç‹¬ç‰¹æ‚£è€…æ•°: {df["RANDID"].nunique()}')

# åˆ†æTIMECVDå’ŒCVDå­—æ®µçš„å…³ç³»ï¼ˆå•ä½ï¼šå¤©ï¼‰
print(f'\nğŸ“Š 24å¹´éšè®¿ç”Ÿå­˜æ•°æ®åˆ†æ:')
print(f'ç ”ç©¶æ€»æ—¶é•¿: {8766}å¤© = {8766/365.25:.1f}å¹´')
print(f'æœªæ‚£ç—…æ‚£è€… (TIMECVD=8766å¤©, CVD=0): {((df["TIMECVD"]==8766) & (df["CVD"]==0)).sum()}')
print(f'æ‚£ç—…æ‚£è€… (TIMECVD<8766å¤©, CVD=1): {((df["TIMECVD"]<8766) & (df["CVD"]==1)).sum()}')
print(f'24å¹´éšè®¿CVDå‘ç—…ç‡: {(df["CVD"]==1).mean():.3f}')

# è½¬æ¢æ—¶é—´å•ä½ä¸ºå¹´
df['time_to_event_years'] = df['TIMECVD'] / 365.25
print(f'\nâ±ï¸ è¯¦ç»†æ—¶é—´åˆ†æ:')
print(f'æœ€é•¿éšè®¿æ—¶é—´: {df["time_to_event_years"].max():.1f} å¹´')

# æ‚£ç—…æ‚£è€…çš„æ—¶é—´åˆ†å¸ƒ
cvd_patients = df[df['CVD'] == 1]
avg_time_to_cvd = cvd_patients['time_to_event_years'].mean()
median_time_to_cvd = cvd_patients['time_to_event_years'].median()
print(f'CVDå¹³å‡å‘ç—…æ—¶é—´: {avg_time_to_cvd:.1f} å¹´')
print(f'CVDä¸­ä½å‘ç—…æ—¶é—´: {median_time_to_cvd:.1f} å¹´')

# åˆ†æä¸åŒæ—¶é—´æ®µçš„å‘ç—…æƒ…å†µ
print(f'\nğŸ“ˆ CVDå‘ç—…æ—¶é—´åˆ†å¸ƒ:')
time_bins = [0, 5, 10, 15, 20, 25]
cvd_time_dist = pd.cut(cvd_patients['time_to_event_years'], bins=time_bins, include_lowest=True)
print(cvd_time_dist.value_counts().sort_index())

# ä½¿ç”¨ç¬¬ä¸€æ¬¡ä½“æ£€æ•°æ®è¿›è¡Œå»ºæ¨¡
print(f'\nğŸ“‹ å‡†å¤‡å»ºæ¨¡æ•°æ® (ä½¿ç”¨ç¬¬ä¸€æ¬¡ä½“æ£€æ•°æ®):')
baseline_data = df[df['PERIOD'] == 1].copy()

# åˆ›å»ºç”Ÿå­˜åˆ†æçš„ç›®æ ‡å˜é‡
baseline_data['event'] = baseline_data['CVD']  # æ˜¯å¦å‘ç”ŸCVDäº‹ä»¶
baseline_data['time_years'] = baseline_data['TIMECVD'] / 365.25  # æ—¶é—´ï¼ˆå¹´ï¼‰

print(f'åŸºçº¿æ•°æ®æ ·æœ¬æ•°: {len(baseline_data)}')
print(f'CVDäº‹ä»¶å‘ç”Ÿæ•°: {baseline_data["event"].sum()}')
print(f'åˆ å¤±æ•°æ®æ•°: {(baseline_data["event"]==0).sum()}')
print(f'CVDå‘ç—…ç‡: {baseline_data["event"].mean():.3f}')

# é€‰æ‹©é¢„æµ‹ç‰¹å¾ï¼ˆæ’é™¤æ—¢å¾€CVDç›¸å…³ç‰¹å¾ï¼Œå› ä¸ºè¿™æ˜¯é¢„æµ‹æ¨¡å‹ï¼‰
risk_factors = [
    'SEX', 'AGE', 'TOTCHOL', 'SYSBP', 'DIABP', 
    'CURSMOKE', 'CIGPDAY', 'BMI', 'DIABETES', 
    'BPMEDS', 'HEARTRTE', 'GLUCOSE'
]

available_features = [col for col in risk_factors if col in baseline_data.columns]
X = baseline_data[available_features].copy()
y = baseline_data['event']  # é¢„æµ‹24å¹´å†…æ˜¯å¦ä¼šå‘ç”ŸCVDäº‹ä»¶

# ç‰¹å¾å·¥ç¨‹
print(f'\nğŸ”§ ç‰¹å¾å·¥ç¨‹:')
if 'SYSBP' in X.columns and 'DIABP' in X.columns:
    X['PULSE_PRESSURE'] = X['SYSBP'] - X['DIABP']
    print('+ è„‰å‹å·® (æ”¶ç¼©å‹-èˆ’å¼ å‹)')

if 'TOTCHOL' in X.columns and 'AGE' in X.columns:
    X['CHOL_AGE_RATIO'] = X['TOTCHOL'] / (X['AGE'] + 1)
    print('+ èƒ†å›ºé†‡å¹´é¾„æ¯”')

# å¹´é¾„åˆ†ç»„
if 'AGE' in X.columns:
    X['AGE_GROUP'] = pd.cut(X['AGE'], bins=[0, 45, 55, 65, 100], labels=[0, 1, 2, 3]).astype(float)
    print('+ å¹´é¾„åˆ†ç»„')

# é«˜è¡€å‹åˆ†ç±»
if 'SYSBP' in X.columns and 'DIABP' in X.columns:
    # æŒ‰ç…§æ ‡å‡†è¡€å‹åˆ†ç±»
    X['HYPERTENSION_STAGE'] = 0  # æ­£å¸¸
    X.loc[(X['SYSBP'] >= 130) | (X['DIABP'] >= 80), 'HYPERTENSION_STAGE'] = 1  # 1æœŸé«˜è¡€å‹
    X.loc[(X['SYSBP'] >= 140) | (X['DIABP'] >= 90), 'HYPERTENSION_STAGE'] = 2  # 2æœŸé«˜è¡€å‹
    print('+ é«˜è¡€å‹åˆ†æœŸ')

# å¸çƒŸé£é™©è¯„åˆ†
if 'CURSMOKE' in X.columns and 'CIGPDAY' in X.columns:
    X['SMOKING_RISK'] = X['CURSMOKE'] * (1 + X['CIGPDAY'].fillna(0) / 20)
    print('+ å¸çƒŸé£é™©è¯„åˆ†')

print(f'æœ€ç»ˆç‰¹å¾æ•°é‡: {X.shape[1]}')
print(f'ç‰¹å¾åˆ—è¡¨: {list(X.columns)}')

# å¤„ç†ç¼ºå¤±å€¼
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)
X_imputed = pd.DataFrame(X_imputed, columns=X.columns)

# æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print(f'\nğŸ¤– è®­ç»ƒ24å¹´CVDé£é™©é¢„æµ‹æ¨¡å‹:')
print(f'è®­ç»ƒé›†å¤§å°: {len(X_train)}')
print(f'æµ‹è¯•é›†å¤§å°: {len(X_test)}')
print(f'è®­ç»ƒé›†CVDç‡: {y_train.mean():.3f}')

# è®­ç»ƒå¤šä¸ªæ¨¡å‹
models = {
    'Logistic_Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random_Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),
}

results = {}
for name, model in models.items():
    print(f'\nğŸ“ˆ è®­ç»ƒ {name}...')
    
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # è¯„ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba)
    
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'auc': auc,
        'y_pred_proba': y_pred_proba
    }
    
    print(f'å‡†ç¡®ç‡: {accuracy:.4f}')
    print(f'AUC: {auc:.4f}')
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print('åˆ†ç±»æŠ¥å‘Š:')
    print(classification_report(y_test, y_pred, target_names=['No CVD', 'CVD'], digits=3))

# é€‰æ‹©æœ€ä½³æ¨¡å‹
best_model_name = max(results.keys(), key=lambda x: results[x]['auc'])
best_model_result = results[best_model_name]

print(f'\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}')
print(f'ğŸ¯ AUCåˆ†æ•°: {best_model_result["auc"]:.4f}')
print(f'ğŸ“ˆ å‡†ç¡®ç‡: {best_model_result["accuracy"]:.4f}')

# ç‰¹å¾é‡è¦æ€§åˆ†æ
if best_model_name == 'Random_Forest':
    feature_importance = best_model_result['model'].feature_importances_
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print(f'\nğŸ“Š ç‰¹å¾é‡è¦æ€§æ’åº:')
    for i, row in importance_df.head(10).iterrows():
        print(f'{row["feature"]}: {row["importance"]:.4f}')
    
    # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
    plt.figure(figsize=(12, 8))
    plt.barh(range(len(importance_df.head(10))), importance_df.head(10)['importance'])
    plt.yticks(range(len(importance_df.head(10))), importance_df.head(10)['feature'])
    plt.xlabel('Feature Importance')
    plt.title('Top 10 Feature Importance for 24-Year CVD Risk Prediction')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig('cvd_24year_feature_importance.png', dpi=300, bbox_inches='tight')
    print('\nç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: cvd_24year_feature_importance.png')

# ä¿å­˜æœ€ä½³æ¨¡å‹
joblib.dump(best_model_result['model'], 'framingham_24year_cvd_model.pkl')
joblib.dump(scaler, 'framingham_24year_scaler.pkl')
joblib.dump(imputer, 'framingham_24year_imputer.pkl')

# ä¿å­˜ç‰¹å¾åç§°
with open('framingham_24year_features.json', 'w') as f:
    json.dump(list(X.columns), f, indent=2)

# ä¿å­˜æ¨¡å‹ä¿¡æ¯
model_info = {
    'model_name': best_model_name,
    'model_type': '24_year_cvd_risk_prediction',
    'auc_score': float(best_model_result['auc']),
    'accuracy': float(best_model_result['accuracy']),
    'features': list(X.columns),
    'description': 'åŸºäºå¼—é›·æ˜æ±‰24å¹´éšè®¿æ•°æ®çš„CVDé£é™©é¢„æµ‹æ¨¡å‹',
    'study_details': {
        'total_patients': int(baseline_data['RANDID'].nunique()),
        'cvd_events': int(baseline_data['event'].sum()),
        'censored': int((baseline_data['event']==0).sum()),
        'follow_up_years': 24.0,
        'avg_time_to_cvd_years': float(avg_time_to_cvd),
        'median_time_to_cvd_years': float(median_time_to_cvd)
    }
}

with open('framingham_24year_model_info.json', 'w', encoding='utf-8') as f:
    json.dump(model_info, f, ensure_ascii=False, indent=2)

print(f'\nğŸ’¾ æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜:')
print('- framingham_24year_cvd_model.pkl (24å¹´CVDé£é™©é¢„æµ‹æ¨¡å‹)')
print('- framingham_24year_scaler.pkl (æ ‡å‡†åŒ–å™¨)')
print('- framingham_24year_imputer.pkl (ç¼ºå¤±å€¼å¤„ç†å™¨)')
print('- framingham_24year_features.json (ç‰¹å¾åç§°)')
print('- framingham_24year_model_info.json (æ¨¡å‹ä¿¡æ¯)')

print(f'\nğŸ’¡ å¼—é›·æ˜æ±‰24å¹´éšè®¿ç ”ç©¶æ€»ç»“:')
print('1. è¿™æ˜¯ä¸€ä¸ª24å¹´é•¿æœŸéšè®¿çš„å‰ç»æ€§é˜Ÿåˆ—ç ”ç©¶')
print('2. TIMECVDå•ä½ä¸ºå¤©ï¼Œ8766å¤© = 24å¹´')
print('3. å¹³å‡CVDå‘ç—…æ—¶é—´çº¦12å¹´ï¼Œä¸­ä½å‘ç—…æ—¶é—´çº¦{:.1f}å¹´'.format(median_time_to_cvd))
print('4. æ¨¡å‹å¯é¢„æµ‹æ‚£è€…24å¹´å†…CVDå‘ç—…é£é™©')
print('5. è¿™æ˜¯çœŸæ­£çš„é•¿æœŸå¿ƒè¡€ç®¡ç–¾ç—…é£é™©è¯„ä¼°å·¥å…·')
print('6. é€‚ç”¨äºä¸´åºŠé•¿æœŸé£é™©åˆ†å±‚å’Œé¢„é˜²å†³ç­–')

print(f'\nâœ… 24å¹´CVDé£é™©é¢„æµ‹æ¨¡å‹æ„å»ºå®Œæˆ!') 